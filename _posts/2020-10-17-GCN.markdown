---
layout:     post
title:      "Graph representation learning"
subtitle:   " \" Graph convolution \""
date:       2020-10-20 12:00:00
author:     "Kzou"
header-img: "img/3d_mesh.jpg"
catalog: true
tags:
    - Tech
    - Graph
---

> ‚Äú My first tech blog‚Äù

## Introduction

In our daily life, the most common media we see are images, videos and sounds. One feature of this data is that they are neatly arranged. For example, the pixels of an image can be represented by a matrix which is an an underlying Euclidean or grid-like structure (Fig 1.). Thanks to this property, convolutional neural networks have had great success on 2d picture. (Fig 2.). <br>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/euclidean_data_structure.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 1. Euclidean data structure</div>
</center>


<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/convolution%20on%202d%20image.jpg">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 2. Convolution on 2d image</div>
</center>

<!--<img src="/img/euclidean_data_structure.png" title="Fig 1. Euclidean data structure" width="400" height="100" />-->
<!--[](/img/euclidean_data_structure.png)-->


However, many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. (Fig 3.)

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/social_network.jpg">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 3. Social network graph</div>
</center>


In a lot of application of such dataset, such geometric datasets are large and complex, we should consider the feature of each node as well as the connectivity. Anyway, graph is a nature targets for machine learning techology, since important real-world datasets come in the form of graphs or networks. In particular, we would like to use deep learning.
Fortunately, more and more researches show that it's feasible to apply convolution neural network on the graph and even proved that it's a powerful tool in graph representation learning. The graph convolution is divided into two main categories -- spectral method and space method.

<!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>-->

## Spectral method
We define a mesh as a set of vertices and edges, $G=(V, A)$.  where $V$ represents the set of vertices in Euclidean space. A is a sparse $(0, 1)-matrix$, if i-th point and
j-th point are connected, $A_{i j} = 1$, otherwise, $A_{i j} = 0$. It shows how the points in the graph are connected, so it's called an adjacency matrix. Given a simple graph $G$ with n vertices, its Laplacien matrix $L_{n \times n}$ is defined as $L=D-A$, where $D$ is the degree matrix, which represent the degree of each vertices in graph $G$ and is computed by $D_{i i}=\sum_{j} A_{i j}$. <br>
The Laplacian matrix can be decomposed as $L=U \Lambda U^{T}$, as it‚Äôs a symmetry matrix. Where $U=\left(\overrightarrow{u_{1}}, \overrightarrow{u_{2}}, \ldots, \overrightarrow{u_{n}}\right)$ is a matrix with column vectors as the eigenvectors of $L$. $\Lambda=\operatorname{diag}\left(\left[\lambda_{0}, \lambda_{1}, \ldots, \lambda_{n}\right]\right)$ is a diagonal matrix of n eigen values. 


**Fourier Transform on the graph** We know the traditional Fourier Transform is defined as
$$F(\omega)=\mathcal{F}[f(t)]=\int f(t) e^{-i \omega t} d t$$

It‚Äôs the integral of the signal $f(t)$ with the basis function $e^{-i \omega t}$. Mathematically, $e^{-i \omega t}$ is the eigne function of the Laplace operator and $\omega$ is related to the eigen value as $\Delta e^{-i \omega t} = -\omega^{2} e^{-i \omega t}$. With reference to this, while dealing with Graph problems, we can introduce the Laplace
matrices to define graph Fourier transform. In another word, we just need to find the eigen function and eigen value of laplacien matrix. Then we can obtain:
$$F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i}^{N} f(i) u_{l}^{*}(i)$$

Where $f$ is an n-dimension vectors on the graph, and f(i) corresponds one-to-one with the vertices of the graph. And $ùë¢_l(ùëñ)$ represents the $ith$ component of the $ith$ eignvector. Then the Fourier transform of $f$ of the eigenvalue $ùúÜ$ is an inner product operation of $f$ with the eigenvector $ùë¢_ùëô$ corresponding to $ùúÜ_ùëô$ which can be written in the form of a matrix product.

$$
\left(\begin{array}{c}
\hat{f}\left(\lambda_{1}\right) \\
\hat{f}\left(\lambda_{2}\right) \\
\vdots \\
\hat{f}\left(\lambda_{N}\right)
\end{array}\right)=\left(\begin{array}{cccc}
u_{1}(1) & u_{1}(2) & \ldots & u_{1}(N) \\
u_{2}(1) & u_{2}(2) & \ldots & u_{2}(N) \\
\vdots & \vdots & \ddots & \vdots \\
u_{N}(1) & u_{N}(2) & \ldots & u_{N}(N)
\end{array}\right)\left(\begin{array}{c}
f(1) \\
f(2) \\
\vdots \\
f(N)
\end{array}\right)
$$

The matrix form of the Fourier transform of f on Graph is $\hat{f}=U^{T} f$.
Then we can easily get the inverse Fourier transform of f which is $f= ùëà\hat{f}$


**Graph convolution.** After obtaining the graph convolution, we then can calculate the graph convolution based on spectral domain according to convolution theorem. We introduce a convolutional kernel $h$ to make it convolute with the graph $f$, then we have:

$(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)$




on the graph:

## Spatial method
