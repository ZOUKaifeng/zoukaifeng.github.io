---
layout:     post
title:      "Graph representation learning"
subtitle:   " \" part 1: Graph convolution -- spectral method\""
date:       2020-10-20 12:00:00
author:     "Kzou"
header-img: "img/3d_mesh.jpg"
catalog: true
tags:
    - Tech
    - Graph
---

> â€œ My first tech blog â€

## Introduction

In our daily life, the most common media we see are images, videos and sounds. One feature of this data is that they are neatly arranged. For example, the pixels of an image can be represented by a matrix which is an an underlying Euclidean or grid-like structure (Fig 1.). Thanks to this property, convolutional neural networks have had great success on 2d picture. (Fig 2.). <br>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/euclidean_data_structure.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 1. Euclidean data structure</div>
</center>


<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/convolution%20on%202d%20image.jpg">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 2. Convolution on 2d image</div>
</center>

<!--<img src="/img/euclidean_data_structure.png" title="Fig 1. Euclidean data structure" width="400" height="100" />-->
<!--[](/img/euclidean_data_structure.png)-->


However, many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. (Fig 3.)

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/social_network.jpg">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 3. Social network graph</div>
</center>


In a lot of application of such dataset, such geometric datasets are large and complex, we should consider the feature of each node as well as the connectivity. Anyway, graph is a nature targets for machine learning techology, since important real-world datasets come in the form of graphs or networks. In particular, we would like to use deep learning.
Fortunately, more and more researches show that it's feasible to apply convolution neural network on the graph and even proved that it's a powerful tool in graph representation learning. The graph convolution is divided into two main categories -- spectral method and space method.

<!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>-->

## Spectral method
We define a mesh as a set of vertices and edges, $G=(V, A)$.  where $V$ represents the set of vertices in Euclidean space. A is a sparse $(0, 1)-matrix$, if i-th point and
j-th point are connected, $A_{i j} = 1$, otherwise, $A_{i j} = 0$. It shows how the points in the graph are connected, so it's called an adjacency matrix. Given a simple graph $G$ with n vertices, its Laplacien matrix $L_{n \times n}$ is defined as $L=D-A$, where $D$ is the degree matrix, which represent the degree of each vertices in graph $G$ and is computed by $D_{i i}=\sum_{j} A_{i j}$. <br>
The Laplacian matrix can be decomposed as $L=U \Lambda U^{T}$, as itâ€™s a symmetry matrix. Where $U=\left(\overrightarrow{u_{1}}, \overrightarrow{u_{2}}, \ldots, \overrightarrow{u_{n}}\right)$ is a matrix with column vectors as the eigenvectors of $L$. $\Lambda=\operatorname{diag}\left(\left[\lambda_{0}, \lambda_{1}, \ldots, \lambda_{n}\right]\right)$ is a diagonal matrix of n eigen values. 


**Fourier Transform on the graph.** We know the traditional Fourier Transform is defined as

$$ F(\omega)=\mathcal{F}[f(t)]=\int f(t) e^{-i \omega t} d t $$

Itâ€™s the integral of the signal $f(t)$ with the basis function $e^{-i \omega t}$. Mathematically, $e^{-i \omega t}$ is the eigne function of the Laplace operator and $\omega$ is related to the eigen value as $\Delta e^{-i \omega t} = -\omega^{2} e^{-i \omega t}$. With reference to this, while dealing with Graph problems, we can introduce the Laplace
matrices to define graph Fourier transform. In another word, we just need to find the eigen function and eigen value of laplacien matrix. Then we can obtain:

$$ F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i}^{N} f(i) u_{l}^{ * }(i)$$

Where $f$ is an n-dimension vectors on the graph, and f(i) corresponds one-to-one with the vertices of the graph. And $ğ‘¢_l(ğ‘–)$ represents the $ith$ component of the $ith$ eignvector. Then the Fourier transform of $f$ of the eigenvalue $ğœ†$ is an inner product operation of $f$ with the eigenvector $ğ‘¢_ğ‘™$ corresponding to $ğœ†_ğ‘™$ which can be written in the form of a matrix product.

$$
\left(\begin{array}{c}
\hat{f}\left(\lambda_{1}\right) \\
\hat{f}\left(\lambda_{2}\right) \\
\vdots \\
\hat{f}\left(\lambda_{N}\right)
\end{array}\right)=\left(\begin{array}{cccc}
u_{1}(1) & u_{1}(2) & \ldots & u_{1}(N) \\
u_{2}(1) & u_{2}(2) & \ldots & u_{2}(N) \\
\vdots & \vdots & \ddots & \vdots \\
u_{N}(1) & u_{N}(2) & \ldots & u_{N}(N)
\end{array}\right)\left(\begin{array}{c}
f(1) \\
f(2) \\
\vdots \\
f(N)
\end{array}\right)
$$

The matrix form of the Fourier transform of f on Graph is $\hat{f}=U^{T} f$.
Then we can easily get the inverse Fourier transform of f which is $f= ğ‘ˆ\hat{f}$

### Graph convolution[1].

After obtaining the graph convolution, we then can calculate the graph convolution based on spectral domain according to **convolution theorem**. We introduce a convolutional kernel $h$ to make it convolute with the graph $f$, then we have:

$$(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)$$


Where $ğ‘ˆ^{ğ‘‡}â„$, $ğ‘ˆ^{ğ‘‡}ğ‘“$ represent the Fourier transform of the convolutional kernel and the vertices of graph respectively, then the convolution operator * can be defined as an inverse Fourier transform of a Hadamard product.


**Chebyshev graph convolution[2].** The above equation based on convolution theorom is computational expensive with a large number of vertices, because the kernels are global and have a large number of parameters. The kernel size is the same as the input signal. Such a kernel would require a lot of computational power if applied to deep learning. 
The problem is solved by using recursive Chebyshev polynomials to formulate mesh filtering with a kernel $â„_{\theta}$. 

$$
\hat{h}(\Lambda)=\sum_{k=0}^{N} \beta_{k} T_{k}(\hat{\Lambda})
$$

Where $\hat{\Lambda}$ is the eigenvalue. $ğ›½_ğ‘˜$ is a vector of the Chebyshev coefficients. After replacing the convolutional kernel with a Chebyshev polynomial we haveï¼š

$$
(f * h)_{G}=\sum_{k=0}^{N} \beta_{k} T_{k}\left(U \Lambda U^{T}\right) f=\sum_{k=0}^{N} \beta_{k} T_{k}(\hat{L}) f
$$


Where $\hat{L}=\frac{2}{\lambda_{\max }} L-I_{N}$ ğ‘ is the scaled Laplacian. $ğ‘‡_ğ‘˜$ is the Chebyshev polynomial of order $k$ that can be computed recursively as $T_{k}(x)=2 x T_{k-1}(x)-T_{k-2}(x)$ with $T_0 = 1, T_1 = x$.  $ğ›½_ğ‘˜$ represent the trainable parameters with shape ğ¹ğ‘–ğ‘› Ã— ğ¹ğ‘œğ‘¢ğ‘¡. Thus, the convolutional kernel has only K learnable parameters, and in general K is much less than N which is the number of vertices. The complexity of the convolution is great reduced. 

## Code



**Reference**
[1] Henaff, M., Bruna, J., LeCun, Y.: Deep convolutional networks on graph-structured data. CoRR abs/1506.05163 (2015)
[2] Defferrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on graphs with fast localized spectral filtering. In: Advances in Neural Information Processing Systems. pp. 3844â€“3852 (2016)
[3] MM Bronstein, J Bruna, Y LeCun, A Szlam, P Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine 34 (4), 18-42.
